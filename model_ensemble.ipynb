{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import requests\n",
    "import xmltodict\n",
    "from PIL import Image, ImageDraw\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from math import *\n",
    "import shutil\n",
    "import copy\n",
    "from torch.utils.checkpoint import checkpoint \n",
    "import time\n",
    "from log import *\n",
    "from torch.utils.data import Dataset\n",
    "import xml.etree.ElementTree as et\n",
    "from torchvision import datasets,transforms,utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.models import * \n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=20\n",
    "file_name=\"test\"\n",
    "if not os.path.isdir(\"./ensemble_result\"):\n",
    "    os.mkdir(\"./ensemble_result\")\n",
    "Path=\"./ensemble_result/\"+file_name+\"/\"\n",
    "log_path=Path+\"Process_log.txt\"\n",
    "batch_size=256\n",
    "\n",
    "lr=0.005                  \n",
    "step_size=10\n",
    "gamma=0.1\n",
    "\n",
    "\n",
    "if os.path.isdir(Path):\n",
    "    print(\"이미 폴더가 존재합니다.\")\n",
    "    sys.exit()\n",
    "os.mkdir(Path)\n",
    "\n",
    "print(\"log_path :\",log_path)\n",
    "TXT=log(log_path)\n",
    "TXT.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation 필요, 단일 데이터 라벨링 이기 때문에 다중 데이터 같은 경우 bounding box 에서 에러 날 수 있으니 확인\n",
    "\n",
    "def Data_Pretreatment(path=\"./166.약품식별 인공지능 개발을 위한 경구약제 이미지 데이터/01.데이터/1.Training/\",\n",
    "                      category=[\"object_name\",\"RHF\",\"RVF\",\"RG\",\"path\"],\n",
    "                      img_move_path=\"./DATA/\",\n",
    "                      img_move=0,\n",
    "                      data_augmentation={\"RHF\":1,\"RVF\":1,\"RG\":1}):\n",
    "    \n",
    "    if os.path.isfile(\"./dataframe.csv\"):\n",
    "        DATAFRAME=pd.read_csv(\"./dataframe.csv\")\n",
    "        yshape=DATAFRAME.shape[0]\n",
    "        object_type=[i for i in range(DATAFRAME[\"object_name\"].iloc[yshape-1]+1)]\n",
    "        return pd.read_csv(\"./dataframe.csv\"),object_type\n",
    "    \n",
    "    print(\"data pretreatment\")\n",
    "\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(p=0.5),\n",
    "    #transforms.RandomGrayscale(p=0.75),\n",
    "\n",
    "    label_path=path+\"라벨링데이터/\"\n",
    "    image_path=path+\"원천데이터/\"\n",
    "    if img_move==1:\n",
    "        if not os.path.isdir(\"./DATA\"):\n",
    "            os.mkdir(\"./DATA\")\n",
    "\n",
    "    dataframe=np.array([])\n",
    "    dataframe = np.empty((0,len(category)), int)\n",
    "    \n",
    "    img_draw_stack=0\n",
    "    object_type=[]\n",
    "    object_type_idx=0\n",
    "\n",
    "\n",
    "    for big_category in os.listdir(image_path):\n",
    "          \n",
    "        big_category_path=image_path+big_category+\"/\"\n",
    "\n",
    "        for mid_category in os.listdir(big_category_path):\n",
    "\n",
    "            if len(mid_category.split(\".zip\"))>=2:continue\n",
    "            mid_category_path=big_category_path+mid_category+\"/\"\n",
    "            \n",
    "            for object_name in os.listdir(mid_category_path):\n",
    "\n",
    "                #print(\"object name :\",object_name)\n",
    "\n",
    "                object_path=mid_category_path+object_name+\"/\"\n",
    "                #print(os.listdir(object_path))\n",
    "\n",
    "                for file_name in os.listdir(object_path):\n",
    "\n",
    "                    img2json_path=file_name.split(\".\")[0]+\".json\"\n",
    "                    mid_category_replace=mid_category.replace(\"S\",\"L\")\n",
    "                    object_label_path=label_path+big_category+\"/\"+mid_category_replace+\"/\"+object_name+\"_json\"+\"/\"+img2json_path\n",
    "                    with open (object_label_path, \"r\",encoding='UTF-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    NAME=data['images'][0]['dl_name'].replace(\"/\",\"-\")\n",
    "                    object_file_path=img_move_path+NAME+\"/\"\n",
    "\n",
    "                    \n",
    "\n",
    "                \n",
    "\n",
    "        \n",
    "                    if not NAME in object_type:\n",
    "                        object_type.append(NAME)\n",
    "                        object_type_idx+=1\n",
    "\n",
    "                    \"\"\"\n",
    "                    if img_draw_stack==0:\n",
    "                        img_draw_stack=1\n",
    "\n",
    "\n",
    "                        img = Image.open(object_file_path+file_name).convert('RGB')\n",
    "                        Size=[data['images'][0]['width'],data['images'][0]['height']]\n",
    "                        img=img.resize((256,256))\n",
    "                        img.show()\n",
    "                        BOX=data['annotations'][0]['bbox']\n",
    "                        alpha=[256/Size[0],256/Size[1]]\n",
    "                        BOX=[BOX[i]*alpha[i%2]for i in range(4)]\n",
    "                        draw = ImageDraw.Draw(img)\n",
    "                        draw.rectangle((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]), outline=(0,255,0), width = 3)\n",
    "\n",
    "                        img.show()\n",
    "                        sys.exit()\n",
    "                    \n",
    "                    \"\"\"\n",
    "\n",
    "                    Size=[data['images'][0]['width'],data['images'][0]['height']]\n",
    "                    BOX=data['annotations'][0]['bbox']\n",
    "\n",
    "                    if img_move==1:\n",
    "                        img_size=(480,480)\n",
    "                        img = Image.open(object_path+file_name).convert('RGB')\n",
    "                        Size=img.size\n",
    "                        if not os.path.isdir(object_file_path):\n",
    "                            os.mkdir(object_file_path)\n",
    "                        center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "                        img=img.crop((center[0]-240,center[1]-240,center[0]+240,center[1]+240))\n",
    "                        img=img.resize((240,240))\n",
    "                        img.save(object_file_path+file_name,\"PNG\")\n",
    "                        #shutil.copyfile(object_path+file_name, object_file_path+file_name)\n",
    "                    dataframe=np.append(dataframe,np.array([[object_type_idx-1,0,0,0,object_file_path+file_name]]),axis=0)\n",
    "                    if data_augmentation[\"RHF\"]:\n",
    "                        dataframe=np.append(dataframe,np.array([[object_type_idx-1,1,0,0,object_file_path+file_name]]),axis=0)\n",
    "                    if data_augmentation[\"RVF\"]:\n",
    "                        dataframe=np.append(dataframe,np.array([[object_type_idx-1,0,1,0,object_file_path+file_name]]),axis=0)\n",
    "                    if data_augmentation[\"RG\"]:\n",
    "                        dataframe=np.append(dataframe,np.array([[object_type_idx-1,0,0,1,object_file_path+file_name]]),axis=0)\n",
    "                    #print(NAME,BOX)\n",
    "\n",
    "    DATAFRAME=pd.DataFrame(data=dataframe,columns=category)\n",
    "    #print(\"DATAFRAME_TYPE :\",type(DATAFRAME)\n",
    "    # )\n",
    "\n",
    "    return DATAFRAME,object_type\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataFrame_dividing(DataFrame,category=[\"object_name\",\"path\"]):\n",
    "\n",
    "    train=np.array([])\n",
    "    train = np.empty((0,len(category)), int)\n",
    "    test=np.array([])\n",
    "    test = np.empty((0,len(category)), int)\n",
    "\n",
    "    #print(DataFrame)\n",
    "    \n",
    "\n",
    "    class_stack=[]\n",
    "    last_obj=-1\n",
    "\n",
    "    #print(DataFrame.shape)\n",
    "    #print(DATAFRAME)\n",
    "\n",
    "    for i in range(DataFrame.shape[0]):\n",
    "\n",
    "        Ip=[]\n",
    "        for g in range(len(category)):Ip.append(DataFrame.iloc[i][category[g]])\n",
    "        if last_obj==-1:\n",
    "            last_obj=Ip[0]\n",
    "\n",
    "        #print(last_obj,Ip[0])\n",
    "\n",
    "        if last_obj!=Ip[0]:\n",
    "            #print(\"##\")\n",
    "            T=int(len(class_stack)/10 * 7)\n",
    "            #print(\"TT\",T)\n",
    "            for g in range(T):\n",
    "                train=np.append(train,np.array([class_stack[g]]),axis=0)\n",
    "                #print(train,np.array([class_stack[g]]),class_stack[g])\n",
    "                #np.append(dataframe,np.array([[object_type_idx-1, BOX[0], BOX[1], BOX[0]+BOX[2], BOX[1]+BOX[3], object_file_path+file_name]]),axis=0)\n",
    "            for g in range(T,len(class_stack)):\n",
    "                test=np.append(test,np.array([class_stack[g]]),axis=0)\n",
    "\n",
    "            class_stack=[]\n",
    "            last_obj=Ip[0]\n",
    "        class_stack.append(Ip)\n",
    "\n",
    "    if class_stack!=[]:\n",
    "        T=int(len(class_stack)/10 * 7)\n",
    "        for g in range(T):\n",
    "            train=np.append(train,np.array([class_stack[g]]),axis=0)\n",
    "            #np.append(dataframe,np.array([[object_type_idx-1, BOX[0], BOX[1], BOX[0]+BOX[2], BOX[1]+BOX[3], object_file_path+file_name]]),axis=0)\n",
    "        for g in range(T,len(class_stack)):\n",
    "            test=np.append(test,np.array([class_stack[g]]),axis=0)\n",
    "        \n",
    "    train=pd.DataFrame(data=train,columns=category)\n",
    "    test=pd.DataFrame(data=test,columns=category)\n",
    "\n",
    "    return train,test\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransForm(size=256):\n",
    "    size=256\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.Resize(size),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomGrayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "    ])\n",
    "    return train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,labels,transforms):\n",
    "        self.labels=labels\n",
    "        #print(self.labels)\n",
    "        self.transforms=transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path=self.labels[\"path\"].iloc[idx]\n",
    "        label=self.labels[\"object_name\"].iloc[idx]\n",
    "        #print(idx,self.labels.iloc[idx])\n",
    "\n",
    "        img=Image.open(img_path)\n",
    "\n",
    "        if self.labels[\"RHF\"].iloc[idx]:\n",
    "            hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "            img=hflipper(img)\n",
    "\n",
    "        elif self.labels[\"RVF\"].iloc[idx]:\n",
    "            vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "            img=vflipper(img)\n",
    "\n",
    "        elif self.labels[\"RG\"].iloc[idx]:\n",
    "            scaler = transforms.RandomGrayscale(p=0.75)\n",
    "            img=scaler(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            img=self.transforms(img)\n",
    "\n",
    "       # OUTPUT={\"iamge\":img,\"label\":label}\n",
    "\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation 필요, 단일 데이터 라벨링 이기 때문에 다중 데이터 같은 경우 bounding box 에서 에러 날 수 있으니 확인\n",
    "#image folder produce data file\n",
    "def if_pd_datafile(path=\"./166.약품식별 인공지능 개발을 위한 경구약제 이미지 데이터/01.데이터/1.Training/\",\n",
    "                      category=[\"object_name\",\"RHF\",\"RVF\",\"RG\",\"path\"],\n",
    "                      img_move_path=\"./DATA/\",\n",
    "                      data_augmentation={\"RHF\":1,\"RVF\":1,\"RG\":1}):\n",
    "    \n",
    "\n",
    "    label_path=path+\"라벨링데이터/\"\n",
    "    image_path=path+\"원천데이터/\"\n",
    "    if not os.path.isdir(\"./DATA\"):\n",
    "         os.mkdir(\"./DATA\")\n",
    "    if not os.path.isdir(\"./DATA/train\"):\n",
    "         os.mkdir(\"./DATA/train\")\n",
    "    if not os.path.isdir(\"./DATA/test\"):\n",
    "         os.mkdir(\"./DATA/test\")\n",
    "\n",
    "    class_stack=[]\n",
    "    \n",
    "    object_type=[]\n",
    "    object_type_idx=0\n",
    "\n",
    "\n",
    "    for big_category in os.listdir(image_path):\n",
    "          \n",
    "        big_category_path=image_path+big_category+\"/\"\n",
    "\n",
    "        for mid_category in os.listdir(big_category_path):\n",
    "\n",
    "            if len(mid_category.split(\".zip\"))>=2:continue\n",
    "            mid_category_path=big_category_path+mid_category+\"/\"\n",
    "            \n",
    "            for object_name in os.listdir(mid_category_path):\n",
    "\n",
    "                #print(\"object name :\",object_name)\n",
    "\n",
    "                object_path=mid_category_path+object_name+\"/\"\n",
    "                #print(os.listdir(object_path))\n",
    "\n",
    "                for file_name in os.listdir(object_path):\n",
    "\n",
    "                    img2json_path=file_name.split(\".\")[0]+\".json\"\n",
    "                    mid_category_replace=mid_category.replace(\"S\",\"L\")\n",
    "                    object_label_path=label_path+big_category+\"/\"+mid_category_replace+\"/\"+object_name+\"_json\"+\"/\"+img2json_path\n",
    "                    with open (object_label_path, \"r\",encoding='UTF-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    NAME=data['images'][0]['dl_name'].replace(\"/\",\"-\")\n",
    "        \n",
    "                    if not NAME in object_type:\n",
    "                        if object_type!=[]:\n",
    "                            T=int(len(class_stack)/10 * 7)\n",
    "                            #train\n",
    "                            for g in range(T):\n",
    "                                DATA=class_stack[g]\n",
    "                                BOX=DATA[4]\n",
    "\n",
    "                                img = Image.open(DATA[3]).convert('RGB')\n",
    "                                object_file_path=img_move_path+\"train/\"+DATA[2]+\"/\"\n",
    "                                if not os.path.isdir(object_file_path):\n",
    "                                    os.mkdir(object_file_path)\n",
    "\n",
    "                                center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "                                #img=img.crop((center[0]-240,center[1]-240,center[0]+240,center[1]+240))\n",
    "                                img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "                                #print(BOX)\n",
    "                                #print(img.size)\n",
    "                                img=img.resize((240,240))\n",
    "\n",
    "                                if DATA[1][\"RHF\"]:\n",
    "                                    hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                                    img=hflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RVF\"]:\n",
    "                                    vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                                    img=vflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RG\"]:\n",
    "                                    scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                                    img=scaler(img)\n",
    "\n",
    "                                img.save(object_file_path+DATA[5],\"PNG\")\n",
    "\n",
    "\n",
    "                            #test\n",
    "                            for g in range(T,len(class_stack)):\n",
    "                                DATA=class_stack[g]\n",
    "                                BOX=DATA[4]\n",
    "\n",
    "                                img = Image.open(DATA[3]).convert('RGB')\n",
    "                                object_file_path=img_move_path+\"test/\"+DATA[2]+\"/\"\n",
    "                                if not os.path.isdir(object_file_path):\n",
    "                                    os.mkdir(object_file_path)\n",
    "\n",
    "                                center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "                                img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "                                img=img.resize((240,240))\n",
    "\n",
    "                                if DATA[1][\"RHF\"]:\n",
    "                                    hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                                    img=hflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RVF\"]:\n",
    "                                    vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                                    img=vflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RG\"]:\n",
    "                                    scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                                    img=scaler(img)\n",
    "\n",
    "                                img.save(object_file_path+DATA[5],\"PNG\")\n",
    "\n",
    "                        object_type.append(NAME)\n",
    "                        class_stack=[]\n",
    "                        object_type_idx+=1\n",
    "\n",
    "                    \n",
    "                    class_stack.append([object_type_idx,data_augmentation,NAME,object_path+file_name,data['annotations'][0]['bbox'],file_name])\n",
    "                    \n",
    "                \n",
    "                    #print(NAME,BOX)\n",
    "    if class_stack!=[]:\n",
    "        T=int(len(class_stack)/10 * 7)\n",
    "        #train\n",
    "        for g in range(T):\n",
    "            DATA=class_stack[g]\n",
    "            BOX=DATA[4]\n",
    "\n",
    "            img = Image.open(DATA[3]).convert('RGB')\n",
    "            object_file_path=img_move_path+\"train/\"+DATA[2]+\"/\"\n",
    "            if not os.path.isdir(object_file_path):\n",
    "                os.mkdir(object_file_path)\n",
    "\n",
    "            center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "            img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "\n",
    "            img=img.resize((240,240))\n",
    "            if DATA[1][\"RHF\"]:\n",
    "                hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                img=hflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RVF\"]:\n",
    "                vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                img=vflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RG\"]:\n",
    "                scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                img=scaler(img)\n",
    "\n",
    "            img.save(object_file_path+DATA[5],\"PNG\")\n",
    "\n",
    "\n",
    "        #test\n",
    "        for g in range(T,len(class_stack)):\n",
    "            DATA=class_stack[g]\n",
    "            BOX=DATA[4]\n",
    "\n",
    "            img = Image.open(DATA[3]).convert('RGB')\n",
    "            object_file_path=img_move_path+\"test/\"+DATA[2]+\"/\"\n",
    "            if not os.path.isdir(object_file_path):\n",
    "                os.mkdir(object_file_path)\n",
    "\n",
    "            center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "            img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "            img=img.resize((240,240))\n",
    "\n",
    "            if DATA[1][\"RHF\"]:\n",
    "                hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                img=hflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RVF\"]:\n",
    "                vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                img=vflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RG\"]:\n",
    "                scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                img=scaler(img)\n",
    "\n",
    "            img.save(object_file_path+DATA[5],\"PNG\")\n",
    "            \n",
    "\n",
    "    return object_type\n",
    "\n",
    "    #DATAFRAME=pd.DataFrame(data=dataframe,columns=category)\n",
    "    #print(\"DATAFRAME_TYPE :\",type(DATAFRAME)\n",
    "    # )\n",
    "\n",
    "    #return DATAFRAME,object_type\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_Dataset(train_dataset,test_dataset,train_transform,test_transform,batch_size=64):\n",
    "\n",
    "  #[train,train_transform]\n",
    "  #print(\"train\")\n",
    "  train_dataset = CustomDataset(train_dataset,train_transform)\n",
    "  #print(\"test\")\n",
    "  test_dataset = CustomDataset(test_dataset,test_transform)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "  return {\"dataset\":train_dataset,\"dataloader\":train_dataloader},{\"dataset\":test_dataset,\"dataloader\":test_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IF_produce_Dataset(train_transform,test_transform,batch_size=64):\n",
    "\n",
    "  train_dataset = datasets.ImageFolder(os.path.join(\"./DATA/train\"),train_transform)\n",
    "\n",
    "  test_dataset = datasets.ImageFolder(os.path.join(\"./DATA/test\"),test_transform)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "  return {\"dataset\":train_dataset,\"dataloader\":train_dataloader},{\"dataset\":test_dataset,\"dataloader\":test_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_print(input):\n",
    "    TXT.print_log(input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(TEST,PATH):\n",
    "\n",
    "    \"\"\"\n",
    "    model_name=[['resnet50','resnet50_v2.pth'],['resnet101','resnet101_v1.pth'],['resnet101','resnet101_v2.pth'],['resnet152','resnet152_v2.pth']]\n",
    "    model_weights=['ResNet50_Weights.IMAGENET1K_V2','ResNet101_Weights.IMAGENET1K_V1','ResNet101_Weights.IMAGENET1K_V2','ResNet152_Weights.IMAGENET1K_V2']\n",
    "    \"\"\"\n",
    "\n",
    "    #sys.stdout=open(PATH+\"train_result.txt\",\"w\")\n",
    "    since = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    sum_running_loss = 0.0\n",
    "    sum_running_corrects = 0.0 #[0]*(len(MPI.object_stack)+2)\n",
    "\n",
    "    max_running_loss = 0.0\n",
    "    max_running_corrects = 0.0\n",
    "\n",
    "    # 데이터를 반복\n",
    "    for i,batch in enumerate(TEST[\"learning_dataloader\"]):\n",
    "        #print(\"idx\",i)\n",
    "\n",
    "        inputs,labels=batch\n",
    "        inputs = inputs.to(device)\n",
    "        #labels=torch.tensor(labels)\n",
    "        labels = torch.LongTensor.clone(labels).to(device)\n",
    "\n",
    "        sum_loss=0.0\n",
    "        max_loss=0.0\n",
    "        sum_stack=[]\n",
    "        max_stack=[]\n",
    "        for idx in range(len(TEST['model_name'])):\n",
    "            print(idx)\n",
    "\n",
    "            if TEST[\"model_name\"][idx][0]=='resnet50':\n",
    "                model_conv = resnet50(weights=TEST['model_weights'][0])#(weights=\"ResNet152_Weights.IMAGENET1K_V1\")\n",
    "            elif TEST[\"model_name\"][idx][0]=='resnet101_v1':\n",
    "                model_conv = resnet101(weights=TEST['model_weights'][1])\n",
    "            elif TEST[\"model_name\"][idx][0]=='resnet101_v2':\n",
    "                model_conv = resnet101(weights=TEST['model_weights'][2])\n",
    "            elif TEST[\"model_name\"][idx][0]=='resnet152':\n",
    "                model_conv = resnet152(weights=TEST['model_weights'][3])\n",
    "\n",
    "            for param in model_conv.parameters():param.requires_grad = False\n",
    "            num_ftrs = model_conv.fc.in_features\n",
    "            model_conv.fc = nn.Linear(num_ftrs, len(TEST['object_type']))\n",
    "            model_conv = model_conv.to(device)\n",
    "            print(TEST[\"model_name\"][idx][0],PATH+TEST['model_name'][idx][1])\n",
    "            TL=torch.load(PATH+TEST['model_name'][idx][1])\n",
    "            model_conv.load_state_dict(TL)\n",
    "            model_conv.eval()\n",
    "            #print(idx)\n",
    "\n",
    "            outputs = model_conv(inputs)\n",
    "            #print(torch.min(outputs))\n",
    "            outputs+=abs(torch.min(outputs).item())+0.01\n",
    "            #print(torch.min(outputs))\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            max_stack.append(preds.detach().cpu().numpy().tolist())\n",
    "            sum_stack.append(outputs)\n",
    "            AA=TEST[\"criterion\"](outputs, labels)\n",
    "            max_loss += AA\n",
    "            sum_loss += AA\n",
    "\n",
    "        max_stack=max_stack[0]\n",
    "        #print(\"stack\",stack)\n",
    "        #print(len(stack))\n",
    "\n",
    "        max_loss/=len(TEST['model_name'])\n",
    "        sum_loss/=len(TEST['model_name'])\n",
    "        ANS=[[0 for i in range(7)]for g in range(len(max_stack))]\n",
    "        #print(\"ans\",ANS)\n",
    "        #print(len(ANS))\n",
    "        Su=sum_stack[0]\n",
    "        #print(stack)\n",
    "        #print(0,Su)\n",
    "        for g in range(len(max_stack)):\n",
    "            ANS[g][int(max_stack[g])]+=1\n",
    "\n",
    "        for i in range(1,len(sum_stack)):\n",
    "            Su+=sum_stack[i]\n",
    "            #print(i,Su)\n",
    "        max_preds=torch.Tensor([ANS[i].index(max(ANS[i]))for i in range(len(ANS))]).cuda()\n",
    "        _, sum_preds = torch.max(Su, 1)\n",
    "        #print(\"preds\",preds)\n",
    "        #print(len(preds))\n",
    "\n",
    "        #print(\"finish\",Su)\n",
    "\n",
    "        # 통계\n",
    "        sum_running_loss += sum_loss.item() * inputs.size(0)\n",
    "        sum_running_corrects += torch.sum(sum_preds == labels.data)\n",
    "\n",
    "        max_running_loss += max_loss.item() * inputs.size(0)\n",
    "        max_running_corrects += torch.sum(max_preds == labels.data)\n",
    "\n",
    "    \n",
    "    TXT.close()\n",
    "    TXT.open()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    TXT.print_log(f'ensemble Testing complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    TXT.print_log(\"\")\n",
    "    TXT.print_log(f'max test Acc: {max_running_corrects.double() / TEST[\"learning_dataset_size\"]}')\n",
    "    TXT.print_log(f'max test Acc\\'s loss : {max_running_loss / TEST[\"learning_dataset_size\"]}')\n",
    "    TXT.print_log(f'sum test Acc: {sum_running_corrects.double() / TEST[\"learning_dataset_size\"]}')\n",
    "    TXT.print_log(f'sum test Acc\\'s loss : {sum_running_loss / TEST[\"learning_dataset_size\"]}')\n",
    "\n",
    "    TXT.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "TXT.print_log(\"Hyperparameter setting\")\n",
    "TXT.print_log(\"----------------------\")\n",
    "\n",
    "\n",
    "\n",
    "#Hyperparameter_path=Path+\"Hyperparameter.txt\"\n",
    "#sys.stdout=open(Hyperparameter_path,\"w\")\n",
    "\n",
    "#---------------------Dataset setting---------------------#\n",
    "\n",
    "category=[\"object_name\",\"RHF\",\"RVF\",\"RG\",\"path\"]\n",
    "#object_type=if_pd_datafile(category=category)\n",
    "object_type=[0]*5\n",
    "train_transform,test_transform=TransForm(240)\n",
    "\"\"\"\n",
    "DATAFRAME,object_type=Data_Pretreatment(category=category,img_move=0)\n",
    "train,test=DataFrame_dividing(DATAFRAME,category)\n",
    "\n",
    "#print(\"test\")\n",
    "#print(test)\n",
    "\n",
    "Train,Test=produce_Dataset(train,test,train_transform,test_transform,batch_size)\n",
    "\"\"\"\n",
    "Train,Test=IF_produce_Dataset(train_transform,test_transform,batch_size)\n",
    "\n",
    "learning_dataset={\"train\":Train[\"dataset\"],\"test\":Test[\"dataset\"]}\n",
    "learning_dataloader={\"train\":Train[\"dataloader\"],\"test\":Test[\"dataloader\"]}\n",
    "learning_dataset_size={\"train\":len(Train[\"dataset\"]),\"test\":len(Test[\"dataset\"])}\n",
    "\n",
    "#-----\n",
    "# ---------------Dataset setting---------------------#\n",
    "\n",
    "\"\"\"\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "dataiter = iter(learning_dataloader[\"train\"])\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(utils.make_grid(images))\n",
    "# 정답(label) 출력\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
    "\"\"\"\n",
    "\n",
    "#---------------------model---------------------#\n",
    "model_name=[['resnet50','resnet50_v2.pth']]#,['resnet101_v1','resnet101_v1.pth'],['resnet101','resnet101_v2.pth']]#,['resnet152','resnet152_v2.pth']]\n",
    "#'resnet50_v1.pth','resnet152_v1.pth'\n",
    "model_weights=['ResNet50_Weights.IMAGENET1K_V2','ResNet101_Weights.IMAGENET1K_V1','ResNet101_Weights.IMAGENET1K_V2','ResNet152_Weights.IMAGENET1K_V2']\n",
    "weights=[1,1,1]\n",
    "#resnet\n",
    "#num_ftrs = model_conv.fc.in_features\n",
    "#model_conv.fc = nn.Linear(num_ftrs, len(object_type))\n",
    "\n",
    "\n",
    "#densenet\n",
    "#num_ftrs = model_conv.fc.in_features\n",
    "#model_conv.classifier = nn.Linear(num_ftrs, len(object_type))\n",
    "\n",
    "#efficientnet\n",
    "#num_ftrs = model_conv.classifier[1].in_features\n",
    "#model_conv.classifier[1] = nn.Linear(num_ftrs, len(object_type))\n",
    "\n",
    "#model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    #TXT.print_log(str(\"Loss Function is CrossEntropyLoss\"))\n",
    "\n",
    "\n",
    "\n",
    "#---------------------model---------------------#\n",
    "#TXT.print_log(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model, criterion, optimizer, scheduler, learning_dataloader,learning_dataset_size,EPOCH,PATH)\n",
    "TXT.print_log(\"train & test log\")\n",
    "TXT.print_log(\"---------\")\n",
    "TEST={\n",
    "       \"criterion\":criterion,\n",
    "        \"learning_dataloader\":learning_dataloader[\"test\"],\n",
    "        \"learning_dataset_size\":learning_dataset_size[\"test\"],\n",
    "        \"model_name\":model_name,\n",
    "        \"model_weights\":model_weights,\n",
    "        \"object_type\":object_type,\n",
    "        \"weights\":weights\n",
    "}\n",
    "\n",
    "test_model(TEST,'./ensemble_file/')\n",
    "TXT.open()\n",
    "TXT.print_log(\"---------------------------\")\n",
    "TXT.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_dataset_size[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
