{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경구약 안전 복용 서비스(Medicine Safe Dose Service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인공지능 학습과정에서 필요한 함수들을 정의한다.\n",
    "import pandas as pd # -> DataFrame 을 생성하기 위한 함수\n",
    "import json # -> 데이터 셋에 들어 있는 json 파일을 읽기 위한 함수\n",
    "import numpy as np # -> DataFrame 을 생성하는데 쓰이는 함수로, numpy 로 배열을 만든 뒤 DataFrame 에 추가하기 위한 함수\n",
    "import sys # ->\n",
    "from PIL import Image, ImageDraw #->  파일의 이미지를 불러오기 위한 함수\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from math import *\n",
    "import shutil\n",
    "import copy\n",
    "from torch.utils.checkpoint import checkpoint \n",
    "import time\n",
    "from log import *\n",
    "from torch.utils.data import Dataset\n",
    "import xml.etree.ElementTree as et\n",
    "from torchvision import datasets,transforms,utils\n",
    "from torch.utils.data import DataLoader\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.models import * \n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import os\n",
    "# -> cuda(gpu)를 사용할 때 에러가 날 경우 에러를 자세히 보기 위한 코드\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import gc\n",
    "# -> 파이썬이 처음 시작할 때 생성하는 많은 객체들을 효율적으로 관리하기 위한 코드\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\white\\.cache\\torch\\hub\\checkpoints\n",
    "사전학습된 모델들이 있는 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습을 위한 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 진행할 때에는 이 코드만 건들면 된다.\n",
    "\n",
    "epoch=10\n",
    "file_name=\"test1\"\n",
    "if not os.path.isdir(\"./result\"):\n",
    "    os.mkdir(\"./result\")\n",
    "Path=\"./result/\"+file_name+\"/\"\n",
    "log_path=Path+\"Process_log.txt\"\n",
    "batch_size=256\n",
    "# lr 0.001 batch size 256\n",
    "\n",
    "lr=0.02\n",
    "step_size=10\n",
    "gamma=0.1\n",
    "\n",
    "\n",
    "if os.path.isdir(Path):\n",
    "    print(\"이미 폴더가 존재합니다.\")\n",
    "    sys.exit()\n",
    "os.mkdir(Path)\n",
    "\n",
    "print(\"log_path :\",log_path)\n",
    "TXT=log(log_path)\n",
    "TXT.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. DATA -> Augmentation -> Train : Test = 7 : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Pretreatment(path=\"./166.약품식별 인공지능 개발을 위한 경구약제 이미지 데이터/01.데이터/1.Training/\",\n",
    "                      category=[\"object_name\",\"RHF\",\"RVF\",\"RG\",\"path\"],\n",
    "                      img_move_path=\"./DATA/\",\n",
    "                      img_move=0,\n",
    "                      data_augmentation={\"RHF\":1,\"RVF\":1,\"RG\":1}):\n",
    "    \n",
    "    if os.path.isfile(\"./dataframe.csv\"):\n",
    "        DATAFRAME=pd.read_csv(\"./dataframe.csv\")\n",
    "        yshape=DATAFRAME.shape[0]\n",
    "        object_type=[i for i in range(DATAFRAME[\"object_name\"].iloc[yshape-1]+1)]\n",
    "        return pd.read_csv(\"./dataframe.csv\"),object_type\n",
    "\n",
    "\n",
    "    label_path=path+\"라벨링데이터/\"\n",
    "    image_path=path+\"원천데이터/\"\n",
    "    if img_move==1:\n",
    "        if not os.path.isdir(\"./DATA\"):\n",
    "            os.mkdir(\"./DATA\")\n",
    "\n",
    "    dataframe=np.array([])\n",
    "    dataframe = np.empty((0,len(category)), int)\n",
    "    \n",
    "    img_draw_stack=0\n",
    "    object_type=[]\n",
    "    object_type_idx=0\n",
    "\n",
    "\n",
    "    for big_category in os.listdir(image_path):\n",
    "          \n",
    "        big_category_path=image_path+big_category+\"/\"\n",
    "\n",
    "        for mid_category in os.listdir(big_category_path):\n",
    "\n",
    "            if len(mid_category.split(\".zip\"))>=2:continue\n",
    "            mid_category_path=big_category_path+mid_category+\"/\"\n",
    "            \n",
    "            for object_name in os.listdir(mid_category_path):\n",
    "\n",
    "                #print(\"object name :\",object_name)\n",
    "\n",
    "                object_path=mid_category_path+object_name+\"/\"\n",
    "                #print(os.listdir(object_path))\n",
    "\n",
    "                for file_name in os.listdir(object_path):\n",
    "\n",
    "                    img2json_path=file_name.split(\".\")[0]+\".json\"\n",
    "                    mid_category_replace=mid_category.replace(\"S\",\"L\")\n",
    "                    object_label_path=label_path+big_category+\"/\"+mid_category_replace+\"/\"+object_name+\"_json\"+\"/\"+img2json_path\n",
    "                    with open (object_label_path, \"r\",encoding='UTF-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    NAME=data['images'][0]['dl_name'].replace(\"/\",\"-\")\n",
    "                    object_file_path=img_move_path+NAME+\"/\"\n",
    "   \n",
    "                    if not NAME in object_type:\n",
    "                        object_type.append(NAME)\n",
    "                        object_type_idx+=1\n",
    "\n",
    "\n",
    "                    Size=[data['images'][0]['width'],data['images'][0]['height']]\n",
    "                    BOX=data['annotations'][0]['bbox']\n",
    "\n",
    "                    if img_move==1:\n",
    "                        img_size=(480,480)\n",
    "                        img = Image.open(object_path+file_name).convert('RGB')\n",
    "                        Size=img.size\n",
    "                        if not os.path.isdir(object_file_path):\n",
    "                            os.mkdir(object_file_path)\n",
    "                        center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "                        img=img.crop((center[0]-240,center[1]-240,center[0]+240,center[1]+240))\n",
    "                        img=img.resize((240,240))\n",
    "                        img.save(object_file_path+file_name,\"PNG\")\n",
    "                        #shutil.copyfile(object_path+file_name, object_file_path+file_name)\n",
    "                    dataframe=np.append(dataframe,np.array([[object_type_idx-1,0,0,0,object_file_path+file_name]]),axis=0)\n",
    "                    if data_augmentation[\"RHF\"]:\n",
    "                        dataframe=np.append(dataframe,np.array([[object_type_idx-1,1,0,0,object_file_path+file_name]]),axis=0)\n",
    "                    if data_augmentation[\"RVF\"]:\n",
    "                        dataframe=np.append(dataframe,np.array([[object_type_idx-1,0,1,0,object_file_path+file_name]]),axis=0)\n",
    "                    if data_augmentation[\"RG\"]:\n",
    "                        dataframe=np.append(dataframe,np.array([[object_type_idx-1,0,0,1,object_file_path+file_name]]),axis=0)\n",
    "                    #print(NAME,BOX)\n",
    "\n",
    "    DATAFRAME=pd.DataFrame(data=dataframe,columns=category)\n",
    "\n",
    "    return DATAFRAME,object_type\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collcet_object_type(path=\"./datafile/\"):\n",
    "\n",
    "    img_path=path\n",
    "\n",
    "    object_type=[]\n",
    "    object_type_idx=0\n",
    "    \n",
    "\n",
    "    for object_name in os.listdir(img_path):\n",
    "\n",
    "        if not object_name in object_type:\n",
    "            object_type.append(object_name)\n",
    "            object_type_idx+=1\n",
    "\n",
    "    return [i for i in range(object_type_idx)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataFrame_dividing(DataFrame,category=[\"object_name\",\"path\"]):\n",
    "\n",
    "    train=np.array([])\n",
    "    train = np.empty((0,len(category)), int)\n",
    "    test=np.array([])\n",
    "    test = np.empty((0,len(category)), int)\n",
    "\n",
    "    #print(DataFrame)\n",
    "    \n",
    "\n",
    "    class_stack=[]\n",
    "    last_obj=-1\n",
    "\n",
    "    #print(DataFrame.shape)\n",
    "    #print(DATAFRAME)\n",
    "\n",
    "    for i in range(DataFrame.shape[0]):\n",
    "\n",
    "        Ip=[]\n",
    "        for g in range(len(category)):Ip.append(DataFrame.iloc[i][category[g]])\n",
    "        if last_obj==-1:\n",
    "            last_obj=Ip[0]\n",
    "\n",
    "        #print(last_obj,Ip[0])\n",
    "\n",
    "        if last_obj!=Ip[0]:\n",
    "            #print(\"##\")\n",
    "            T=int(len(class_stack)/10 * 7)\n",
    "            #print(\"TT\",T)\n",
    "            for g in range(T):\n",
    "                train=np.append(train,np.array([class_stack[g]]),axis=0)\n",
    "                #print(train,np.array([class_stack[g]]),class_stack[g])\n",
    "                #np.append(dataframe,np.array([[object_type_idx-1, BOX[0], BOX[1], BOX[0]+BOX[2], BOX[1]+BOX[3], object_file_path+file_name]]),axis=0)\n",
    "            for g in range(T,len(class_stack)):\n",
    "                test=np.append(test,np.array([class_stack[g]]),axis=0)\n",
    "\n",
    "            class_stack=[]\n",
    "            last_obj=Ip[0]\n",
    "        class_stack.append(Ip)\n",
    "\n",
    "    if class_stack!=[]:\n",
    "        T=int(len(class_stack)/10 * 7)\n",
    "        for g in range(T):\n",
    "            train=np.append(train,np.array([class_stack[g]]),axis=0)\n",
    "            #np.append(dataframe,np.array([[object_type_idx-1, BOX[0], BOX[1], BOX[0]+BOX[2], BOX[1]+BOX[3], object_file_path+file_name]]),axis=0)\n",
    "        for g in range(T,len(class_stack)):\n",
    "            test=np.append(test,np.array([class_stack[g]]),axis=0)\n",
    "        \n",
    "    train=pd.DataFrame(data=train,columns=category)\n",
    "    test=pd.DataFrame(data=test,columns=category)\n",
    "\n",
    "    return train,test\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation 필요, 단일 데이터 라벨링 이기 때문에 다중 데이터 같은 경우 bounding box 에서 에러 날 수 있으니 확인\n",
    "#image folder produce data file\n",
    "def if_pd_datafile(path=\"./166.약품식별 인공지능 개발을 위한 경구약제 이미지 데이터/01.데이터/1.Training/\",\n",
    "                      category=[\"object_name\",\"RHF\",\"RVF\",\"RG\",\"path\"],\n",
    "                      img_move_path=\"./DATA/\",\n",
    "                      data_augmentation={\"RHF\":1,\"RVF\":1,\"RG\":1}):\n",
    "    \n",
    "\n",
    "    label_path=path+\"라벨링데이터/\"\n",
    "    image_path=path+\"원천데이터/\"\n",
    "    if not os.path.isdir(\"./DATA\"):\n",
    "         os.mkdir(\"./DATA\")\n",
    "    if not os.path.isdir(\"./DATA/train\"):\n",
    "         os.mkdir(\"./DATA/train\")\n",
    "    if not os.path.isdir(\"./DATA/test\"):\n",
    "         os.mkdir(\"./DATA/test\")\n",
    "\n",
    "    class_stack=[]\n",
    "    \n",
    "    object_type=[]\n",
    "    object_type_idx=0\n",
    "\n",
    "\n",
    "    for big_category in os.listdir(image_path):\n",
    "          \n",
    "        big_category_path=image_path+big_category+\"/\"\n",
    "\n",
    "        for mid_category in os.listdir(big_category_path):\n",
    "\n",
    "            if len(mid_category.split(\".zip\"))>=2:continue\n",
    "            mid_category_path=big_category_path+mid_category+\"/\"\n",
    "            \n",
    "            for object_name in os.listdir(mid_category_path):\n",
    "\n",
    "                #print(\"object name :\",object_name)\n",
    "\n",
    "                object_path=mid_category_path+object_name+\"/\"\n",
    "                #print(os.listdir(object_path))\n",
    "\n",
    "                for file_name in os.listdir(object_path):\n",
    "\n",
    "                    img2json_path=file_name.split(\".\")[0]+\".json\"\n",
    "                    mid_category_replace=mid_category.replace(\"S\",\"L\")\n",
    "                    object_label_path=label_path+big_category+\"/\"+mid_category_replace+\"/\"+object_name+\"_json\"+\"/\"+img2json_path\n",
    "                    with open (object_label_path, \"r\",encoding='UTF-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    NAME=data['images'][0]['dl_name'].replace(\"/\",\"-\")\n",
    "        \n",
    "                    if not NAME in object_type:\n",
    "                        if object_type!=[]:\n",
    "                            T=int(len(class_stack)/10 * 7)\n",
    "                            #train\n",
    "                            for g in range(T):\n",
    "                                DATA=class_stack[g]\n",
    "                                BOX=DATA[4]\n",
    "\n",
    "                                img = Image.open(DATA[3]).convert('RGB')\n",
    "                                object_file_path=img_move_path+\"train/\"+DATA[2]+\"/\"\n",
    "                                if not os.path.isdir(object_file_path):\n",
    "                                    os.mkdir(object_file_path)\n",
    "\n",
    "                                center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "                                #img=img.crop((center[0]-240,center[1]-240,center[0]+240,center[1]+240))\n",
    "                                img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "                                #print(BOX)\n",
    "                                #print(img.size)\n",
    "                                img=img.resize((240,240))\n",
    "\n",
    "                                if DATA[1][\"RHF\"]:\n",
    "                                    hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                                    img=hflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RVF\"]:\n",
    "                                    vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                                    img=vflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RG\"]:\n",
    "                                    scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                                    img=scaler(img)\n",
    "\n",
    "                                img.save(object_file_path+DATA[5],\"PNG\")\n",
    "\n",
    "\n",
    "                            #test\n",
    "                            for g in range(T,len(class_stack)):\n",
    "                                DATA=class_stack[g]\n",
    "                                BOX=DATA[4]\n",
    "\n",
    "                                img = Image.open(DATA[3]).convert('RGB')\n",
    "                                object_file_path=img_move_path+\"test/\"+DATA[2]+\"/\"\n",
    "                                if not os.path.isdir(object_file_path):\n",
    "                                    os.mkdir(object_file_path)\n",
    "\n",
    "                                center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "                                img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "                                img=img.resize((240,240))\n",
    "\n",
    "                                if DATA[1][\"RHF\"]:\n",
    "                                    hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                                    img=hflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RVF\"]:\n",
    "                                    vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                                    img=vflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RG\"]:\n",
    "                                    scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                                    img=scaler(img)\n",
    "\n",
    "                                img.save(object_file_path+DATA[5],\"PNG\")\n",
    "\n",
    "                        object_type.append(NAME)\n",
    "                        class_stack=[]\n",
    "                        object_type_idx+=1\n",
    "\n",
    "                    \n",
    "                    class_stack.append([object_type_idx,data_augmentation,NAME,object_path+file_name,data['annotations'][0]['bbox'],file_name])\n",
    "                    \n",
    "                \n",
    "                    #print(NAME,BOX)\n",
    "    if class_stack!=[]:\n",
    "        T=int(len(class_stack)/10 * 7)\n",
    "        #train\n",
    "        for g in range(T):\n",
    "            DATA=class_stack[g]\n",
    "            BOX=DATA[4]\n",
    "\n",
    "            img = Image.open(DATA[3]).convert('RGB')\n",
    "            object_file_path=img_move_path+\"train/\"+DATA[2]+\"/\"\n",
    "            if not os.path.isdir(object_file_path):\n",
    "                os.mkdir(object_file_path)\n",
    "\n",
    "            center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "            img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "\n",
    "            img=img.resize((240,240))\n",
    "            if DATA[1][\"RHF\"]:\n",
    "                hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                img=hflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RVF\"]:\n",
    "                vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                img=vflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RG\"]:\n",
    "                scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                img=scaler(img)\n",
    "\n",
    "            img.save(object_file_path+DATA[5],\"PNG\")\n",
    "\n",
    "\n",
    "        #test\n",
    "        for g in range(T,len(class_stack)):\n",
    "            DATA=class_stack[g]\n",
    "            BOX=DATA[4]\n",
    "\n",
    "            img = Image.open(DATA[3]).convert('RGB')\n",
    "            object_file_path=img_move_path+\"test/\"+DATA[2]+\"/\"\n",
    "            if not os.path.isdir(object_file_path):\n",
    "                os.mkdir(object_file_path)\n",
    "\n",
    "            center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "            img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "            img=img.resize((240,240))\n",
    "\n",
    "            if DATA[1][\"RHF\"]:\n",
    "                hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                img=hflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RVF\"]:\n",
    "                vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                img=vflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RG\"]:\n",
    "                scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                img=scaler(img)\n",
    "\n",
    "            img.save(object_file_path+DATA[5],\"PNG\")\n",
    "            \n",
    "\n",
    "    return object_type\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3. Train & Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransForm(size=256):\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.Resize(size),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomGrayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "    ])\n",
    "    return train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,labels,transforms):\n",
    "        self.labels=labels\n",
    "        #print(self.labels)\n",
    "        self.transforms=transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path=self.labels[\"path\"].iloc[idx]\n",
    "        label=self.labels[\"object_name\"].iloc[idx]\n",
    "        #print(idx,self.labels.iloc[idx])\n",
    "\n",
    "        img=Image.open(img_path)\n",
    "\n",
    "        if self.labels[\"RHF\"].iloc[idx]:\n",
    "            hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "            img=hflipper(img)\n",
    "\n",
    "        elif self.labels[\"RVF\"].iloc[idx]:\n",
    "            vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "            img=vflipper(img)\n",
    "\n",
    "        elif self.labels[\"RG\"].iloc[idx]:\n",
    "            scaler = transforms.RandomGrayscale(p=0.75)\n",
    "            img=scaler(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            img=self.transforms(img)\n",
    "\n",
    "       # OUTPUT={\"iamge\":img,\"label\":label}\n",
    "\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_Dataset(train_dataset,test_dataset,train_transform,test_transform,batch_size=64):\n",
    "\n",
    "  #[train,train_transform]\n",
    "  #print(\"train\")\n",
    "  train_dataset = CustomDataset(train_dataset,train_transform)\n",
    "  #print(\"test\")\n",
    "  test_dataset = CustomDataset(test_dataset,test_transform)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "  return {\"dataset\":train_dataset,\"dataloader\":train_dataloader},{\"dataset\":test_dataset,\"dataloader\":test_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IF_produce_Dataset(train_transform,test_transform,batch_size=64):\n",
    "\n",
    "  train_dataset = datasets.ImageFolder(os.path.join(\"./DATA/train\"),train_transform)\n",
    "\n",
    "  test_dataset = datasets.ImageFolder(os.path.join(\"./DATA/test\"),test_transform)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "  return {\"dataset\":train_dataset,\"dataloader\":train_dataloader},{\"dataset\":test_dataset,\"dataloader\":test_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_print(input):\n",
    "    TXT.print_log(input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(TRAIN,TEST,EPOCH,PATH):\n",
    "\n",
    "\n",
    "    #sys.stdout=open(PATH+\"train_result.txt\",\"w\")\n",
    "    since = time.time()\n",
    "    train_best_acc=0.0\n",
    "    train_best_loss=inf\n",
    "    train_idx=0\n",
    "    test_best_acc=0.0\n",
    "    test_best_loss=inf\n",
    "    test_idx=0\n",
    "    \n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        TXT.print_log(f'Epoch {epoch}/{EPOCH - 1}')\n",
    "        TXT.print_log('-' * 10)\n",
    "\n",
    "        ###############################################\n",
    "        #--------------------train--------------------#\n",
    "        ###############################################\n",
    "\n",
    "        TRAIN[\"model\"].train()  # 모델을 학습 모드로 설정\n",
    "\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0 #[0]*(len(MPI.object_stack)+2)\n",
    "        #print(next(iter(TRAIN[\"learning_dataloader\"])))\n",
    "\n",
    "        # 데이터를 반복\n",
    "        for i,batch in enumerate(TRAIN[\"learning_dataloader\"]):\n",
    "            #print(f\"i : {i}, batch : {batch}.\")\n",
    "\n",
    "            \n",
    "            inputs,labels=batch\n",
    "\n",
    "            inputs = inputs.to(device).cuda()\n",
    "            #print(labels)\n",
    "            #labels=torch.tensor(labels)\n",
    "            labels = torch.LongTensor.clone(labels).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = TRAIN[\"model\"](inputs)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            loss = TRAIN[\"criterion\"](outputs, labels)\n",
    "\n",
    "\n",
    "            TRAIN[\"optimizer\"].zero_grad()\n",
    "            #loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            #\"\"\"\n",
    "            \n",
    "            TRAIN[\"optimizer\"].step()\n",
    "            \n",
    "            # 통계\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        TRAIN[\"scheduler\"].step()\n",
    "\n",
    "        epoch_loss = running_loss / TRAIN[\"learning_dataset_size\"]\n",
    "        epoch_acc = running_corrects.double() / TRAIN[\"learning_dataset_size\"]\n",
    "        \n",
    "        TXT.print_log(f'train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if epoch_acc > train_best_acc:\n",
    "            train_best_acc = epoch_acc\n",
    "            train_idx=epoch+1\n",
    "            train_best_loss=epoch_loss\n",
    "\n",
    "        torch.save(TRAIN[\"model\"].state_dict(), PATH+'checkpoint_epoch_'+str(epoch+1)+'.pth')\n",
    "\n",
    "        ##############################################\n",
    "        #--------------------test--------------------#\n",
    "        ##############################################\n",
    "\n",
    "        TEST[\"model\"].load_state_dict(torch.load(PATH+'checkpoint_epoch_'+str(epoch+1)+'.pth'))\n",
    "\n",
    "        TEST[\"model\"].eval()  # 모델을 학습 모드로 설정\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0 #[0]*(len(MPI.object_stack)+2)\n",
    "\n",
    "        # 데이터를 반복\n",
    "        for i,batch in enumerate(TEST[\"learning_dataloader\"]):\n",
    "\n",
    "            inputs,labels=batch\n",
    "            inputs = inputs.to(device)\n",
    "            #labels=torch.tensor(labels)\n",
    "            labels = torch.LongTensor.clone(labels).to(device)\n",
    "\n",
    "            outputs = TEST[\"model\"](inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = TEST[\"criterion\"](outputs, labels)\n",
    "\n",
    "            # 통계\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / TEST[\"learning_dataset_size\"]\n",
    "        epoch_acc = running_corrects.double() / TEST[\"learning_dataset_size\"]\n",
    "        \n",
    "        TXT.print_log(f'test Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if epoch_acc > test_best_acc:\n",
    "            test_best_acc = epoch_acc\n",
    "            test_idx=epoch+1\n",
    "            test_best_loss=epoch_loss\n",
    "\n",
    "        #TXT.close()\n",
    "        #TXT.open()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    TXT.print_log('')\n",
    "    TXT.print_log(f'Training & Testing complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    TXT.print_log(f'Best train Acc: {train_best_acc}')\n",
    "    TXT.print_log(f'Best train Acc\\'s loss : {train_best_loss}')\n",
    "    TXT.print_log(f'Best train Acc\\'s epoch : {train_idx}')\n",
    "    TXT.print_log(\"\")\n",
    "    TXT.print_log(f'Best test Acc: {test_best_acc}')\n",
    "    TXT.print_log(f'Best test Acc\\'s loss : {test_best_loss}')\n",
    "    TXT.print_log(f'Best test Acc\\'s epoch : {test_idx}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT.print_log(\"Hyperparameter setting\")\n",
    "TXT.print_log(\"----------------------\")\n",
    "\n",
    "#---------------------Dataset setting---------------------#\n",
    "\n",
    "category=[\"object_name\",\"RHF\",\"RVF\",\"RG\",\"path\"]\n",
    "#object_type=if_pd_datafile(category=category)\n",
    "object_type=[0]*5\n",
    "train_transform,test_transform=TransForm(240)\n",
    "\"\"\"\n",
    "DATAFRAME,object_type=Data_Pretreatment(category=category,img_move=0)\n",
    "train,test=DataFrame_dividing(DATAFRAME,category)\n",
    "\n",
    "#print(\"test\")\n",
    "#print(test)\n",
    "\n",
    "Train,Test=produce_Dataset(train,test,train_transform,test_transform,batch_size)\n",
    "\"\"\"\n",
    "Train,Test=IF_produce_Dataset(train_transform,test_transform,batch_size)\n",
    "\n",
    "learning_dataset={\"train\":Train[\"dataset\"],\"test\":Test[\"dataset\"]}\n",
    "learning_dataloader={\"train\":Train[\"dataloader\"],\"test\":Test[\"dataloader\"]}\n",
    "learning_dataset_size={\"train\":len(Train[\"dataset\"]),\"test\":len(Test[\"dataset\"])}\n",
    "\n",
    "# ---------------Dataset setting---------------------#\n",
    "\n",
    "\n",
    "#---------------------model---------------------#\n",
    "model_conv = resnet152(weights=\"ResNet152_Weights.IMAGENET1K_V2\")#(weights=\"ResNet152_Weights.IMAGENET1K_V1\")\n",
    "TXT.print_log(str(f\"model is resnet18. weights is ResNet152_Weights.IMAGENET1K_V1\"))\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#resnet\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(object_type))\n",
    "\n",
    "#densenet\n",
    "#num_ftrs = model_conv.classifier.in_features\n",
    "#model_conv.classifier = nn.Linear(num_ftrs, len(object_type))\n",
    "\n",
    "#efficientnet\n",
    "#num_ftrs = model_conv.classifier[1].in_features\n",
    "#model_conv.classifier[1] = nn.Linear(num_ftrs, len(object_type))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "TXT.print_log(str(\"Loss Function is CrossEntropyLoss\"))\n",
    "optimizer_conv = optim.Adam(model_conv.fc.parameters(),lr=lr)\n",
    "TXT.print_log(str(f\"optimizer is Adam. lr is {lr}.\"))\n",
    "TXT.print_log(str(f'batch size is {batch_size}.'))\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=step_size, gamma=gamma)\n",
    "TXT.print_log(str(f\"lr_scheduler is StepLR.\"))\n",
    "TXT.print_log(str(f'step size is {step_size}.'))\n",
    "TXT.print_log(str(f'gamma is {gamma}.'))\n",
    "\n",
    "#---------------------model---------------------#\n",
    "TXT.print_log(\"---------------------------\")\n",
    "#TXT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model, criterion, optimizer, scheduler, learning_dataloader,learning_dataset_size,EPOCH,PATH)\n",
    "#TXT.open()\n",
    "TXT.print_log(\"train & test log\")\n",
    "TXT.print_log(\"---------\")\n",
    "TRAIN={\"model\":model_conv,\n",
    "       \"criterion\":criterion,\n",
    "       \"optimizer\":optimizer_conv,\n",
    "       \"scheduler\":exp_lr_scheduler,\n",
    "        \"learning_dataloader\":learning_dataloader[\"train\"],\n",
    "        \"learning_dataset_size\":learning_dataset_size[\"train\"],\n",
    "}\n",
    "TEST={\"model\":model_conv,\n",
    "       \"criterion\":criterion,\n",
    "       \"optimizer\":optimizer_conv,\n",
    "       \"scheduler\":exp_lr_scheduler,\n",
    "        \"learning_dataloader\":learning_dataloader[\"test\"],\n",
    "        \"learning_dataset_size\":learning_dataset_size[\"test\"],\n",
    "}\n",
    "\n",
    "train_test_model(TRAIN,TEST,epoch,Path)\n",
    "\n",
    "TXT.print_log(\"---------------------------\")\n",
    "TXT.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATAFRAME.to_csv('dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
