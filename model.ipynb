{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import requests\n",
    "import xmltodict\n",
    "from PIL import Image, ImageDraw\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from math import *\n",
    "import shutil\n",
    "import copy\n",
    "from torch.utils.checkpoint import checkpoint \n",
    "import time\n",
    "from log import *\n",
    "from torch.utils.data import Dataset\n",
    "import xml.etree.ElementTree as et\n",
    "from torchvision import datasets,transforms,utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.models import * \n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C:\\Users\\white\\.cache\\torch\\hub\\checkpoints\n",
    "사전학습된 모델들이 있는 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_path : ./result/resnet152_v2_00085_100/Process_log.txt\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "file_name=\"resnet152_v2_00085_100\"\n",
    "if not os.path.isdir(\"./result\"):\n",
    "    os.mkdir(\"./result\")\n",
    "Path=\"./result/\"+file_name+\"/\"\n",
    "log_path=Path+\"Process_log.txt\"\n",
    "batch_size=256\n",
    "\n",
    "lr=0.0085             \n",
    "step_size=10\n",
    "gamma=0.1\n",
    "\n",
    "\n",
    "if os.path.isdir(Path):\n",
    "    print(\"이미 폴더가 존재합니다.\")\n",
    "    sys.exit()\n",
    "os.mkdir(Path)\n",
    "\n",
    "print(\"log_path :\",log_path)\n",
    "TXT=log(log_path)\n",
    "TXT.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation 필요, 단일 데이터 라벨링 이기 때문에 다중 데이터 같은 경우 bounding box 에서 에러 날 수 있으니 확인\n",
    "\n",
    "def Data_Pretreatment(path=\"./166.약품식별 인공지능 개발을 위한 경구약제 이미지 데이터/01.데이터/1.Training/\",\n",
    "                      category=[\"object_name\",\"RHF\",\"RVF\",\"RG\",\"path\"],\n",
    "                      img_move_path=\"./DATA/\",\n",
    "                      img_move=0,\n",
    "                      data_augmentation={\"RHF\":1,\"RVF\":1,\"RG\":1}):\n",
    "    \n",
    "    if os.path.isfile(\"./dataframe.csv\"):\n",
    "        DATAFRAME=pd.read_csv(\"./dataframe.csv\")\n",
    "        yshape=DATAFRAME.shape[0]\n",
    "        object_type=[i for i in range(DATAFRAME[\"object_name\"].iloc[yshape-1]+1)]\n",
    "        return pd.read_csv(\"./dataframe.csv\"),object_type\n",
    "    \n",
    "    print(\"data pretreatment\")\n",
    "\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomVerticalFlip(p=0.5),\n",
    "    #transforms.RandomGrayscale(p=0.75),\n",
    "\n",
    "    label_path=path+\"라벨링데이터/\"\n",
    "    image_path=path+\"원천데이터/\"\n",
    "    if img_move==1:\n",
    "        if not os.path.isdir(\"./DATA\"):\n",
    "            os.mkdir(\"./DATA\")\n",
    "\n",
    "    dataframe=np.array([])\n",
    "    dataframe = np.empty((0,len(category)), int)\n",
    "    \n",
    "    img_draw_stack=0\n",
    "    object_type=[]\n",
    "    object_type_idx=0\n",
    "\n",
    "\n",
    "    for big_category in os.listdir(image_path):\n",
    "          \n",
    "        big_category_path=image_path+big_category+\"/\"\n",
    "\n",
    "        for mid_category in os.listdir(big_category_path):\n",
    "\n",
    "            if len(mid_category.split(\".zip\"))>=2:continue\n",
    "            mid_category_path=big_category_path+mid_category+\"/\"\n",
    "            \n",
    "            for object_name in os.listdir(mid_category_path):\n",
    "\n",
    "                #print(\"object name :\",object_name)\n",
    "\n",
    "                object_path=mid_category_path+object_name+\"/\"\n",
    "                #print(os.listdir(object_path))\n",
    "\n",
    "                for file_name in os.listdir(object_path):\n",
    "\n",
    "                    img2json_path=file_name.split(\".\")[0]+\".json\"\n",
    "                    mid_category_replace=mid_category.replace(\"S\",\"L\")\n",
    "                    object_label_path=label_path+big_category+\"/\"+mid_category_replace+\"/\"+object_name+\"_json\"+\"/\"+img2json_path\n",
    "                    with open (object_label_path, \"r\",encoding='UTF-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    NAME=data['images'][0]['dl_name'].replace(\"/\",\"-\")\n",
    "                    object_file_path=img_move_path+NAME+\"/\"\n",
    "\n",
    "                    \n",
    "\n",
    "                \n",
    "\n",
    "        \n",
    "                    if not NAME in object_type:\n",
    "                        object_type.append(NAME)\n",
    "                        object_type_idx+=1\n",
    "\n",
    "                    \"\"\"\n",
    "                    if img_draw_stack==0:\n",
    "                        img_draw_stack=1\n",
    "\n",
    "\n",
    "                        img = Image.open(object_file_path+file_name).convert('RGB')\n",
    "                        Size=[data['images'][0]['width'],data['images'][0]['height']]\n",
    "                        img=img.resize((256,256))\n",
    "                        img.show()\n",
    "                        BOX=data['annotations'][0]['bbox']\n",
    "                        alpha=[256/Size[0],256/Size[1]]\n",
    "                        BOX=[BOX[i]*alpha[i%2]for i in range(4)]\n",
    "                        draw = ImageDraw.Draw(img)\n",
    "                        draw.rectangle((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]), outline=(0,255,0), width = 3)\n",
    "\n",
    "                        img.show()\n",
    "                        sys.exit()\n",
    "                    \n",
    "                    \"\"\"\n",
    "\n",
    "                    Size=[data['images'][0]['width'],data['images'][0]['height']]\n",
    "                    BOX=data['annotations'][0]['bbox']\n",
    "\n",
    "                    if img_move==1:\n",
    "                        img_size=(480,480)\n",
    "                        img = Image.open(object_path+file_name).convert('RGB')\n",
    "                        Size=img.size\n",
    "                        if not os.path.isdir(object_file_path):\n",
    "                            os.mkdir(object_file_path)\n",
    "                        center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "                        img=img.crop((center[0]-240,center[1]-240,center[0]+240,center[1]+240))\n",
    "                        img=img.resize((240,240))\n",
    "                        img.save(object_file_path+file_name,\"PNG\")\n",
    "                        #shutil.copyfile(object_path+file_name, object_file_path+file_name)\n",
    "                    dataframe=np.append(dataframe,np.array([[object_type_idx-1,0,0,0,object_file_path+file_name]]),axis=0)\n",
    "                    if data_augmentation[\"RHF\"]:\n",
    "                        dataframe=np.append(dataframe,np.array([[object_type_idx-1,1,0,0,object_file_path+file_name]]),axis=0)\n",
    "                    if data_augmentation[\"RVF\"]:\n",
    "                        dataframe=np.append(dataframe,np.array([[object_type_idx-1,0,1,0,object_file_path+file_name]]),axis=0)\n",
    "                    if data_augmentation[\"RG\"]:\n",
    "                        dataframe=np.append(dataframe,np.array([[object_type_idx-1,0,0,1,object_file_path+file_name]]),axis=0)\n",
    "                    #print(NAME,BOX)\n",
    "\n",
    "    DATAFRAME=pd.DataFrame(data=dataframe,columns=category)\n",
    "    #print(\"DATAFRAME_TYPE :\",type(DATAFRAME)\n",
    "    # )\n",
    "\n",
    "    return DATAFRAME,object_type\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collcet_object_type(path=\"./datafile/\"):\n",
    "\n",
    "    img_path=path\n",
    "\n",
    "    object_type=[]\n",
    "    object_type_idx=0\n",
    "    \n",
    "\n",
    "    for object_name in os.listdir(img_path):\n",
    "\n",
    "        if not object_name in object_type:\n",
    "            object_type.append(object_name)\n",
    "            object_type_idx+=1\n",
    "\n",
    "    return [i for i in range(object_type_idx)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataFrame_dividing(DataFrame,category=[\"object_name\",\"path\"]):\n",
    "\n",
    "    train=np.array([])\n",
    "    train = np.empty((0,len(category)), int)\n",
    "    test=np.array([])\n",
    "    test = np.empty((0,len(category)), int)\n",
    "\n",
    "    #print(DataFrame)\n",
    "    \n",
    "\n",
    "    class_stack=[]\n",
    "    last_obj=-1\n",
    "\n",
    "    #print(DataFrame.shape)\n",
    "    #print(DATAFRAME)\n",
    "\n",
    "    for i in range(DataFrame.shape[0]):\n",
    "\n",
    "        Ip=[]\n",
    "        for g in range(len(category)):Ip.append(DataFrame.iloc[i][category[g]])\n",
    "        if last_obj==-1:\n",
    "            last_obj=Ip[0]\n",
    "\n",
    "        #print(last_obj,Ip[0])\n",
    "\n",
    "        if last_obj!=Ip[0]:\n",
    "            #print(\"##\")\n",
    "            T=int(len(class_stack)/10 * 7)\n",
    "            #print(\"TT\",T)\n",
    "            for g in range(T):\n",
    "                train=np.append(train,np.array([class_stack[g]]),axis=0)\n",
    "                #print(train,np.array([class_stack[g]]),class_stack[g])\n",
    "                #np.append(dataframe,np.array([[object_type_idx-1, BOX[0], BOX[1], BOX[0]+BOX[2], BOX[1]+BOX[3], object_file_path+file_name]]),axis=0)\n",
    "            for g in range(T,len(class_stack)):\n",
    "                test=np.append(test,np.array([class_stack[g]]),axis=0)\n",
    "\n",
    "            class_stack=[]\n",
    "            last_obj=Ip[0]\n",
    "        class_stack.append(Ip)\n",
    "\n",
    "    if class_stack!=[]:\n",
    "        T=int(len(class_stack)/10 * 7)\n",
    "        for g in range(T):\n",
    "            train=np.append(train,np.array([class_stack[g]]),axis=0)\n",
    "            #np.append(dataframe,np.array([[object_type_idx-1, BOX[0], BOX[1], BOX[0]+BOX[2], BOX[1]+BOX[3], object_file_path+file_name]]),axis=0)\n",
    "        for g in range(T,len(class_stack)):\n",
    "            test=np.append(test,np.array([class_stack[g]]),axis=0)\n",
    "        \n",
    "    train=pd.DataFrame(data=train,columns=category)\n",
    "    test=pd.DataFrame(data=test,columns=category)\n",
    "\n",
    "    return train,test\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransForm(size=256):\n",
    "    size=256\n",
    "    train_transform = transforms.Compose([\n",
    "        #transforms.Resize(size),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomGrayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "    ])\n",
    "    return train_transform, test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,labels,transforms):\n",
    "        self.labels=labels\n",
    "        #print(self.labels)\n",
    "        self.transforms=transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path=self.labels[\"path\"].iloc[idx]\n",
    "        label=self.labels[\"object_name\"].iloc[idx]\n",
    "        #print(idx,self.labels.iloc[idx])\n",
    "\n",
    "        img=Image.open(img_path)\n",
    "\n",
    "        if self.labels[\"RHF\"].iloc[idx]:\n",
    "            hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "            img=hflipper(img)\n",
    "\n",
    "        elif self.labels[\"RVF\"].iloc[idx]:\n",
    "            vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "            img=vflipper(img)\n",
    "\n",
    "        elif self.labels[\"RG\"].iloc[idx]:\n",
    "            scaler = transforms.RandomGrayscale(p=0.75)\n",
    "            img=scaler(img)\n",
    "\n",
    "        if self.transforms:\n",
    "            img=self.transforms(img)\n",
    "\n",
    "       # OUTPUT={\"iamge\":img,\"label\":label}\n",
    "\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation 필요, 단일 데이터 라벨링 이기 때문에 다중 데이터 같은 경우 bounding box 에서 에러 날 수 있으니 확인\n",
    "#image folder produce data file\n",
    "def if_pd_datafile(path=\"./166.약품식별 인공지능 개발을 위한 경구약제 이미지 데이터/01.데이터/1.Training/\",\n",
    "                      category=[\"object_name\",\"RHF\",\"RVF\",\"RG\",\"path\"],\n",
    "                      img_move_path=\"./DATA/\",\n",
    "                      data_augmentation={\"RHF\":1,\"RVF\":1,\"RG\":1}):\n",
    "    \n",
    "\n",
    "    label_path=path+\"라벨링데이터/\"\n",
    "    image_path=path+\"원천데이터/\"\n",
    "    if not os.path.isdir(\"./DATA\"):\n",
    "         os.mkdir(\"./DATA\")\n",
    "    if not os.path.isdir(\"./DATA/train\"):\n",
    "         os.mkdir(\"./DATA/train\")\n",
    "    if not os.path.isdir(\"./DATA/test\"):\n",
    "         os.mkdir(\"./DATA/test\")\n",
    "\n",
    "    class_stack=[]\n",
    "    \n",
    "    object_type=[]\n",
    "    object_type_idx=0\n",
    "\n",
    "\n",
    "    for big_category in os.listdir(image_path):\n",
    "          \n",
    "        big_category_path=image_path+big_category+\"/\"\n",
    "\n",
    "        for mid_category in os.listdir(big_category_path):\n",
    "\n",
    "            if len(mid_category.split(\".zip\"))>=2:continue\n",
    "            mid_category_path=big_category_path+mid_category+\"/\"\n",
    "            \n",
    "            for object_name in os.listdir(mid_category_path):\n",
    "\n",
    "                #print(\"object name :\",object_name)\n",
    "\n",
    "                object_path=mid_category_path+object_name+\"/\"\n",
    "                #print(os.listdir(object_path))\n",
    "\n",
    "                for file_name in os.listdir(object_path):\n",
    "\n",
    "                    img2json_path=file_name.split(\".\")[0]+\".json\"\n",
    "                    mid_category_replace=mid_category.replace(\"S\",\"L\")\n",
    "                    object_label_path=label_path+big_category+\"/\"+mid_category_replace+\"/\"+object_name+\"_json\"+\"/\"+img2json_path\n",
    "                    with open (object_label_path, \"r\",encoding='UTF-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    NAME=data['images'][0]['dl_name'].replace(\"/\",\"-\")\n",
    "        \n",
    "                    if not NAME in object_type:\n",
    "                        if object_type!=[]:\n",
    "                            T=int(len(class_stack)/10 * 7)\n",
    "                            #train\n",
    "                            for g in range(T):\n",
    "                                DATA=class_stack[g]\n",
    "                                BOX=DATA[4]\n",
    "\n",
    "                                img = Image.open(DATA[3]).convert('RGB')\n",
    "                                object_file_path=img_move_path+\"train/\"+DATA[2]+\"/\"\n",
    "                                if not os.path.isdir(object_file_path):\n",
    "                                    os.mkdir(object_file_path)\n",
    "\n",
    "                                center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "                                #img=img.crop((center[0]-240,center[1]-240,center[0]+240,center[1]+240))\n",
    "                                img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "                                #print(BOX)\n",
    "                                #print(img.size)\n",
    "                                img=img.resize((240,240))\n",
    "\n",
    "                                if DATA[1][\"RHF\"]:\n",
    "                                    hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                                    img=hflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RVF\"]:\n",
    "                                    vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                                    img=vflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RG\"]:\n",
    "                                    scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                                    img=scaler(img)\n",
    "\n",
    "                                img.save(object_file_path+DATA[5],\"PNG\")\n",
    "\n",
    "\n",
    "                            #test\n",
    "                            for g in range(T,len(class_stack)):\n",
    "                                DATA=class_stack[g]\n",
    "                                BOX=DATA[4]\n",
    "\n",
    "                                img = Image.open(DATA[3]).convert('RGB')\n",
    "                                object_file_path=img_move_path+\"test/\"+DATA[2]+\"/\"\n",
    "                                if not os.path.isdir(object_file_path):\n",
    "                                    os.mkdir(object_file_path)\n",
    "\n",
    "                                center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "                                img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "                                img=img.resize((240,240))\n",
    "\n",
    "                                if DATA[1][\"RHF\"]:\n",
    "                                    hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                                    img=hflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RVF\"]:\n",
    "                                    vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                                    img=vflipper(img)\n",
    "\n",
    "                                elif DATA[1][\"RG\"]:\n",
    "                                    scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                                    img=scaler(img)\n",
    "\n",
    "                                img.save(object_file_path+DATA[5],\"PNG\")\n",
    "\n",
    "                        object_type.append(NAME)\n",
    "                        class_stack=[]\n",
    "                        object_type_idx+=1\n",
    "\n",
    "                    \n",
    "                    class_stack.append([object_type_idx,data_augmentation,NAME,object_path+file_name,data['annotations'][0]['bbox'],file_name])\n",
    "                    \n",
    "                \n",
    "                    #print(NAME,BOX)\n",
    "    if class_stack!=[]:\n",
    "        T=int(len(class_stack)/10 * 7)\n",
    "        #train\n",
    "        for g in range(T):\n",
    "            DATA=class_stack[g]\n",
    "            BOX=DATA[4]\n",
    "\n",
    "            img = Image.open(DATA[3]).convert('RGB')\n",
    "            object_file_path=img_move_path+\"train/\"+DATA[2]+\"/\"\n",
    "            if not os.path.isdir(object_file_path):\n",
    "                os.mkdir(object_file_path)\n",
    "\n",
    "            center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "            img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "\n",
    "            img=img.resize((240,240))\n",
    "            if DATA[1][\"RHF\"]:\n",
    "                hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                img=hflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RVF\"]:\n",
    "                vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                img=vflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RG\"]:\n",
    "                scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                img=scaler(img)\n",
    "\n",
    "            img.save(object_file_path+DATA[5],\"PNG\")\n",
    "\n",
    "\n",
    "        #test\n",
    "        for g in range(T,len(class_stack)):\n",
    "            DATA=class_stack[g]\n",
    "            BOX=DATA[4]\n",
    "\n",
    "            img = Image.open(DATA[3]).convert('RGB')\n",
    "            object_file_path=img_move_path+\"test/\"+DATA[2]+\"/\"\n",
    "            if not os.path.isdir(object_file_path):\n",
    "                os.mkdir(object_file_path)\n",
    "\n",
    "            center=[(2*BOX[0]+BOX[2])/2,(2*BOX[1]+BOX[3])/2]\n",
    "            img=img.crop((BOX[0],BOX[1],BOX[0]+BOX[2],BOX[1]+BOX[3]))\n",
    "            img=img.resize((240,240))\n",
    "\n",
    "            if DATA[1][\"RHF\"]:\n",
    "                hflipper = transforms.RandomHorizontalFlip(p=0.5)\n",
    "                img=hflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RVF\"]:\n",
    "                vflipper = transforms.RandomVerticalFlip(p=0.5)\n",
    "                img=vflipper(img)\n",
    "\n",
    "            elif DATA[1][\"RG\"]:\n",
    "                scaler = transforms.RandomGrayscale(p=0.75)\n",
    "                img=scaler(img)\n",
    "\n",
    "            img.save(object_file_path+DATA[5],\"PNG\")\n",
    "            \n",
    "\n",
    "    return object_type\n",
    "\n",
    "    #DATAFRAME=pd.DataFrame(data=dataframe,columns=category)\n",
    "    #print(\"DATAFRAME_TYPE :\",type(DATAFRAME)\n",
    "    # )\n",
    "\n",
    "    #return DATAFRAME,object_type\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_Dataset(train_dataset,test_dataset,train_transform,test_transform,batch_size=64):\n",
    "\n",
    "  #[train,train_transform]\n",
    "  #print(\"train\")\n",
    "  train_dataset = CustomDataset(train_dataset,train_transform)\n",
    "  #print(\"test\")\n",
    "  test_dataset = CustomDataset(test_dataset,test_transform)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "  return {\"dataset\":train_dataset,\"dataloader\":train_dataloader},{\"dataset\":test_dataset,\"dataloader\":test_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IF_produce_Dataset(train_transform,test_transform,batch_size=64):\n",
    "\n",
    "  train_dataset = datasets.ImageFolder(os.path.join(\"./DATA/train\"),train_transform)\n",
    "\n",
    "  test_dataset = datasets.ImageFolder(os.path.join(\"./DATA/test\"),test_transform)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "  return {\"dataset\":train_dataset,\"dataloader\":train_dataloader},{\"dataset\":test_dataset,\"dataloader\":test_dataloader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_print(input):\n",
    "    TXT.print_log(input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(TRAIN,TEST,EPOCH,PATH):\n",
    "\n",
    "\n",
    "    #sys.stdout=open(PATH+\"train_result.txt\",\"w\")\n",
    "    since = time.time()\n",
    "    train_best_acc=0.0\n",
    "    train_best_loss=inf\n",
    "    train_idx=0\n",
    "    test_best_acc=0.0\n",
    "    test_best_loss=inf\n",
    "    test_idx=0\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        TXT.print_log(f'Epoch {epoch}/{EPOCH - 1}')\n",
    "        TXT.print_log('-' * 10)\n",
    "\n",
    "        ###############################################\n",
    "        #--------------------train--------------------#\n",
    "        ###############################################\n",
    "\n",
    "        TRAIN[\"model\"].train()  # 모델을 학습 모드로 설정\n",
    "\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0 #[0]*(len(MPI.object_stack)+2)\n",
    "        #print(next(iter(TRAIN[\"learning_dataloader\"])))\n",
    "\n",
    "        # 데이터를 반복\n",
    "        for i,batch in enumerate(TRAIN[\"learning_dataloader\"]):\n",
    "            #print(f\"i : {i}, batch : {batch}.\")\n",
    "\n",
    "            \n",
    "            inputs,labels=batch\n",
    "\n",
    "            inputs = inputs.to(device).cuda()\n",
    "            #print(labels)\n",
    "            #labels=torch.tensor(labels)\n",
    "            labels = torch.LongTensor.clone(labels).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = TRAIN[\"model\"](inputs)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            loss = TRAIN[\"criterion\"](outputs, labels)\n",
    "\n",
    "\n",
    "            TRAIN[\"optimizer\"].zero_grad()\n",
    "            #loss.requires_grad_(True)\n",
    "            loss.backward()\n",
    "            #\"\"\"\n",
    "            \n",
    "            TRAIN[\"optimizer\"].step()\n",
    "            \n",
    "            # 통계\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        TRAIN[\"scheduler\"].step()\n",
    "\n",
    "        epoch_loss = running_loss / TRAIN[\"learning_dataset_size\"]\n",
    "        epoch_acc = running_corrects.double() / TRAIN[\"learning_dataset_size\"]\n",
    "        \n",
    "        TXT.print_log(f'train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if epoch_acc > train_best_acc:\n",
    "            train_best_acc = epoch_acc\n",
    "            train_idx=epoch+1\n",
    "            train_best_loss=epoch_loss\n",
    "\n",
    "        torch.save(TRAIN[\"model\"].state_dict(), PATH+'checkpoint_epoch_'+str(epoch+1)+'.pth')\n",
    "\n",
    "        ##############################################\n",
    "        #--------------------test--------------------#\n",
    "        ##############################################\n",
    "\n",
    "        TEST[\"model\"].load_state_dict(torch.load(PATH+'checkpoint_epoch_'+str(epoch+1)+'.pth'))\n",
    "\n",
    "        TEST[\"model\"].eval()  # 모델을 학습 모드로 설정\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0 #[0]*(len(MPI.object_stack)+2)\n",
    "\n",
    "        # 데이터를 반복\n",
    "        for i,batch in enumerate(TEST[\"learning_dataloader\"]):\n",
    "\n",
    "            inputs,labels=batch\n",
    "            inputs = inputs.to(device)\n",
    "            #labels=torch.tensor(labels)\n",
    "            labels = torch.LongTensor.clone(labels).to(device)\n",
    "\n",
    "            outputs = TEST[\"model\"](inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = TEST[\"criterion\"](outputs, labels)\n",
    "\n",
    "            # 통계\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / TEST[\"learning_dataset_size\"]\n",
    "        epoch_acc = running_corrects.double() / TEST[\"learning_dataset_size\"]\n",
    "        \n",
    "        TXT.print_log(f'test Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if epoch_acc > test_best_acc:\n",
    "            test_best_acc = epoch_acc\n",
    "            test_idx=epoch+1\n",
    "            test_best_loss=epoch_loss\n",
    "\n",
    "        TXT.close()\n",
    "        TXT.open()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "\n",
    "    TXT.print_log('')\n",
    "    TXT.print_log(f'Training & Testing complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    TXT.print_log(f'Best train Acc: {train_best_acc}')\n",
    "    TXT.print_log(f'Best train Acc\\'s loss : {train_best_loss}')\n",
    "    TXT.print_log(f'Best train Acc\\'s epoch : {train_idx}')\n",
    "    TXT.print_log(\"\")\n",
    "    TXT.print_log(f'Best test Acc: {test_best_acc}')\n",
    "    TXT.print_log(f'Best test Acc\\'s loss : {test_best_loss}')\n",
    "    TXT.print_log(f'Best test Acc\\'s epoch : {test_idx}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter setting\n",
      "----------------------\n",
      "model is resnet18. weights is ResNet152_Weights.IMAGENET1K_V1\n",
      "Loss Function is CrossEntropyLoss\n",
      "optimizer is Adam. lr is 0.0085.\n",
      "lr_scheduler is StepLR.\n",
      "step size is 10.\n",
      "gamma is 0.1.\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "TXT.print_log(\"Hyperparameter setting\")\n",
    "TXT.print_log(\"----------------------\")\n",
    "\n",
    "\n",
    "\n",
    "#Hyperparameter_path=Path+\"Hyperparameter.txt\"\n",
    "#sys.stdout=open(Hyperparameter_path,\"w\")\n",
    "\n",
    "#---------------------Dataset setting---------------------#\n",
    "\n",
    "category=[\"object_name\",\"RHF\",\"RVF\",\"RG\",\"path\"]\n",
    "#object_type=if_pd_datafile(category=category)\n",
    "object_type=[0]*5\n",
    "train_transform,test_transform=TransForm(240)\n",
    "\"\"\"\n",
    "DATAFRAME,object_type=Data_Pretreatment(category=category,img_move=0)\n",
    "train,test=DataFrame_dividing(DATAFRAME,category)\n",
    "\n",
    "#print(\"test\")\n",
    "#print(test)\n",
    "\n",
    "Train,Test=produce_Dataset(train,test,train_transform,test_transform,batch_size)\n",
    "\"\"\"\n",
    "Train,Test=IF_produce_Dataset(train_transform,test_transform,batch_size)\n",
    "\n",
    "learning_dataset={\"train\":Train[\"dataset\"],\"test\":Test[\"dataset\"]}\n",
    "learning_dataloader={\"train\":Train[\"dataloader\"],\"test\":Test[\"dataloader\"]}\n",
    "learning_dataset_size={\"train\":len(Train[\"dataset\"]),\"test\":len(Test[\"dataset\"])}\n",
    "\n",
    "#-----\n",
    "# ---------------Dataset setting---------------------#\n",
    "\n",
    "\"\"\"\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "dataiter = iter(learning_dataloader[\"train\"])\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(utils.make_grid(images))\n",
    "# 정답(label) 출력\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
    "\"\"\"\n",
    "\n",
    "#---------------------model---------------------#\n",
    "model_conv = resnet152(weights=\"ResNet152_Weights.IMAGENET1K_V2\")#(weights=\"ResNet152_Weights.IMAGENET1K_V1\")\n",
    "TXT.print_log(str(f\"model is resnet18. weights is ResNet152_Weights.IMAGENET1K_V1\"))\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#resnet\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(object_type))\n",
    "\n",
    "#densenet\n",
    "#num_ftrs = model_conv.classifier.in_features\n",
    "#model_conv.classifier = nn.Linear(num_ftrs, len(object_type))\n",
    "\n",
    "#efficientnet\n",
    "#num_ftrs = model_conv.classifier[1].in_features\n",
    "#model_conv.classifier[1] = nn.Linear(num_ftrs, len(object_type))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "TXT.print_log(str(\"Loss Function is CrossEntropyLoss\"))\n",
    " \n",
    "\n",
    "optimizer_conv = optim.Adam(model_conv.fc.parameters(),lr=lr)\n",
    "TXT.print_log(str(f\"optimizer is Adam. lr is {lr}.\"))\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=step_size, gamma=gamma)\n",
    "TXT.print_log(str(f\"lr_scheduler is StepLR.\"))\n",
    "TXT.print_log(str(f'step size is {step_size}.'))\n",
    "TXT.print_log(str(f'gamma is {gamma}.'))\n",
    "\n",
    "\n",
    "\n",
    "#---------------------model---------------------#\n",
    "TXT.print_log(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train & test log\n",
      "---------\n",
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 4.5542 Acc: 0.0597\n",
      "test Loss: 1.6060 Acc: 0.3402\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 1.3741 Acc: 0.3890\n",
      "test Loss: 1.5094 Acc: 0.1151\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 2.3488 Acc: 0.1834\n",
      "test Loss: 1.3211 Acc: 0.2634\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 1.6431 Acc: 0.3812\n",
      "test Loss: 1.1132 Acc: 0.5166\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 1.1920 Acc: 0.4928\n",
      "test Loss: 1.0846 Acc: 0.5627\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.7634 Acc: 0.7602\n",
      "test Loss: 1.0397 Acc: 0.5934\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.8183 Acc: 0.7028\n",
      "test Loss: 1.0487 Acc: 0.5831\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.8895 Acc: 0.6586\n",
      "test Loss: 0.9659 Acc: 0.6036\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.7239 Acc: 0.7834\n",
      "test Loss: 0.9048 Acc: 0.6598\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.6048 Acc: 0.8298\n",
      "test Loss: 0.8696 Acc: 0.6829\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.5568 Acc: 0.8630\n",
      "test Loss: 0.8331 Acc: 0.7059\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.5429 Acc: 0.8740\n",
      "test Loss: 0.7788 Acc: 0.7468\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.5313 Acc: 0.8895\n",
      "test Loss: 0.7380 Acc: 0.7724\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.5630 Acc: 0.8575\n",
      "test Loss: 0.6991 Acc: 0.8031\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.5381 Acc: 0.8762\n",
      "test Loss: 0.6714 Acc: 0.8286\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.5300 Acc: 0.8773\n",
      "test Loss: 0.6419 Acc: 0.8517\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.5143 Acc: 0.8906\n",
      "test Loss: 0.6230 Acc: 0.8670\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.5276 Acc: 0.8785\n",
      "test Loss: 0.6068 Acc: 0.8875\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.5129 Acc: 0.8939\n",
      "test Loss: 0.5992 Acc: 0.8798\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.5119 Acc: 0.8906\n",
      "test Loss: 0.5983 Acc: 0.8875\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.5209 Acc: 0.8972\n",
      "test Loss: 0.5972 Acc: 0.8900\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.5175 Acc: 0.8674\n",
      "test Loss: 0.5959 Acc: 0.8926\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.5125 Acc: 0.8851\n",
      "test Loss: 0.5980 Acc: 0.8926\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.4825 Acc: 0.9028\n",
      "test Loss: 0.6001 Acc: 0.8900\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.4976 Acc: 0.9094\n",
      "test Loss: 0.6009 Acc: 0.8926\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.5094 Acc: 0.8862\n",
      "test Loss: 0.5969 Acc: 0.9028\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.5000 Acc: 0.8950\n",
      "test Loss: 0.5945 Acc: 0.9079\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.5132 Acc: 0.8851\n",
      "test Loss: 0.5937 Acc: 0.9003\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.5084 Acc: 0.8917\n",
      "test Loss: 0.5918 Acc: 0.9054\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.5179 Acc: 0.8829\n",
      "test Loss: 0.5906 Acc: 0.9028\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.5091 Acc: 0.8895\n",
      "test Loss: 0.5926 Acc: 0.9028\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.5241 Acc: 0.8796\n",
      "test Loss: 0.5934 Acc: 0.9054\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.5134 Acc: 0.8773\n",
      "test Loss: 0.5973 Acc: 0.9028\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.4942 Acc: 0.8950\n",
      "test Loss: 0.5987 Acc: 0.9003\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.5082 Acc: 0.8983\n",
      "test Loss: 0.5947 Acc: 0.8977\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.4979 Acc: 0.8895\n",
      "test Loss: 0.5985 Acc: 0.8951\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.5093 Acc: 0.8950\n",
      "test Loss: 0.5990 Acc: 0.8875\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.5045 Acc: 0.8994\n",
      "test Loss: 0.6033 Acc: 0.8951\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.5126 Acc: 0.8807\n",
      "test Loss: 0.6015 Acc: 0.8875\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.5035 Acc: 0.8895\n",
      "test Loss: 0.5986 Acc: 0.8875\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.5007 Acc: 0.8829\n",
      "test Loss: 0.5948 Acc: 0.8951\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.4987 Acc: 0.8972\n",
      "test Loss: 0.5979 Acc: 0.8900\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.4870 Acc: 0.9072\n",
      "test Loss: 0.6002 Acc: 0.8849\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.5022 Acc: 0.8950\n",
      "test Loss: 0.5976 Acc: 0.8900\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.4952 Acc: 0.9094\n",
      "test Loss: 0.5968 Acc: 0.8951\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.4998 Acc: 0.8983\n",
      "test Loss: 0.5917 Acc: 0.8900\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.5045 Acc: 0.8972\n",
      "test Loss: 0.5894 Acc: 0.8951\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.5220 Acc: 0.8796\n",
      "test Loss: 0.5937 Acc: 0.8926\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.4972 Acc: 0.9017\n",
      "test Loss: 0.5923 Acc: 0.8977\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.5020 Acc: 0.8895\n",
      "test Loss: 0.5931 Acc: 0.8824\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.4806 Acc: 0.8994\n",
      "test Loss: 0.5942 Acc: 0.8747\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.4973 Acc: 0.9039\n",
      "test Loss: 0.5989 Acc: 0.8824\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.5083 Acc: 0.8818\n",
      "test Loss: 0.5987 Acc: 0.8747\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.4895 Acc: 0.8939\n",
      "test Loss: 0.6004 Acc: 0.8772\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.4917 Acc: 0.9017\n",
      "test Loss: 0.5978 Acc: 0.8824\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.5084 Acc: 0.8928\n",
      "test Loss: 0.5961 Acc: 0.8926\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.5053 Acc: 0.9017\n",
      "test Loss: 0.5969 Acc: 0.8926\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.5255 Acc: 0.8906\n",
      "test Loss: 0.5964 Acc: 0.9028\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.5074 Acc: 0.8884\n",
      "test Loss: 0.5942 Acc: 0.8951\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.5097 Acc: 0.8983\n",
      "test Loss: 0.5942 Acc: 0.8977\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.5149 Acc: 0.8895\n",
      "test Loss: 0.5969 Acc: 0.9003\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.5011 Acc: 0.8873\n",
      "test Loss: 0.5969 Acc: 0.8977\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.4998 Acc: 0.8873\n",
      "test Loss: 0.5956 Acc: 0.8926\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.5011 Acc: 0.8884\n",
      "test Loss: 0.5989 Acc: 0.8900\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.5157 Acc: 0.8895\n",
      "test Loss: 0.5966 Acc: 0.9003\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.5113 Acc: 0.9017\n",
      "test Loss: 0.5962 Acc: 0.8977\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.4980 Acc: 0.8840\n",
      "test Loss: 0.5951 Acc: 0.8772\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.5008 Acc: 0.8961\n",
      "test Loss: 0.5952 Acc: 0.8875\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.4827 Acc: 0.9072\n",
      "test Loss: 0.5976 Acc: 0.8900\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.5036 Acc: 0.8928\n",
      "test Loss: 0.6001 Acc: 0.8977\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.5061 Acc: 0.8961\n",
      "test Loss: 0.6025 Acc: 0.8977\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.5121 Acc: 0.8862\n",
      "test Loss: 0.6023 Acc: 0.8977\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.5079 Acc: 0.8851\n",
      "test Loss: 0.5981 Acc: 0.8951\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.4922 Acc: 0.8928\n",
      "test Loss: 0.5956 Acc: 0.9003\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.5116 Acc: 0.8873\n",
      "test Loss: 0.5962 Acc: 0.8926\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.4975 Acc: 0.9094\n",
      "test Loss: 0.5979 Acc: 0.9054\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.5135 Acc: 0.8961\n",
      "test Loss: 0.5983 Acc: 0.9003\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.4941 Acc: 0.8972\n",
      "test Loss: 0.5968 Acc: 0.9003\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.5261 Acc: 0.8785\n",
      "test Loss: 0.5965 Acc: 0.8977\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.5080 Acc: 0.8961\n",
      "test Loss: 0.5976 Acc: 0.8977\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.5055 Acc: 0.8818\n",
      "test Loss: 0.5971 Acc: 0.8951\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.5003 Acc: 0.8740\n",
      "test Loss: 0.5975 Acc: 0.8849\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.4997 Acc: 0.9017\n",
      "test Loss: 0.5998 Acc: 0.8900\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.5071 Acc: 0.8939\n",
      "test Loss: 0.5972 Acc: 0.8849\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.5070 Acc: 0.8917\n",
      "test Loss: 0.5945 Acc: 0.8926\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.5212 Acc: 0.8840\n",
      "test Loss: 0.5934 Acc: 0.8926\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.5141 Acc: 0.8884\n",
      "test Loss: 0.5946 Acc: 0.8849\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.5046 Acc: 0.8785\n",
      "test Loss: 0.5953 Acc: 0.8849\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.4970 Acc: 0.8939\n",
      "test Loss: 0.5954 Acc: 0.8900\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.5012 Acc: 0.8873\n",
      "test Loss: 0.5980 Acc: 0.8926\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.5014 Acc: 0.8917\n",
      "test Loss: 0.5955 Acc: 0.8900\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.4983 Acc: 0.8829\n",
      "test Loss: 0.5950 Acc: 0.8900\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.5000 Acc: 0.8928\n",
      "test Loss: 0.5968 Acc: 0.8951\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.5163 Acc: 0.8785\n",
      "test Loss: 0.6005 Acc: 0.8977\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.5102 Acc: 0.8718\n",
      "test Loss: 0.5992 Acc: 0.8951\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.5037 Acc: 0.8873\n",
      "test Loss: 0.5954 Acc: 0.8926\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.4888 Acc: 0.8983\n",
      "test Loss: 0.5953 Acc: 0.8849\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.5203 Acc: 0.8884\n",
      "test Loss: 0.5948 Acc: 0.8875\n",
      "Epoch 98/99\n",
      "----------\n",
      "train Loss: 0.5045 Acc: 0.9006\n",
      "test Loss: 0.5998 Acc: 0.8824\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.4977 Acc: 0.9083\n",
      "test Loss: 0.5951 Acc: 0.8926\n",
      "\n",
      "Training & Testing complete in 62m 27s\n",
      "Best train Acc: 0.9093922651933702\n",
      "Best train Acc's loss : 0.4976454449622012\n",
      "Best train Acc's epoch : 25\n",
      "\n",
      "Best test Acc: 0.9079283887468031\n",
      "Best test Acc's loss : 0.5944689562558518\n",
      "Best test Acc's epoch : 27\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "#train_model(model, criterion, optimizer, scheduler, learning_dataloader,learning_dataset_size,EPOCH,PATH)\n",
    "\n",
    "TXT.print_log(\"train & test log\")\n",
    "TXT.print_log(\"---------\")\n",
    "TRAIN={\"model\":model_conv,\n",
    "       \"criterion\":criterion,\n",
    "       \"optimizer\":optimizer_conv,\n",
    "       \"scheduler\":exp_lr_scheduler,\n",
    "        \"learning_dataloader\":learning_dataloader[\"train\"],\n",
    "        \"learning_dataset_size\":learning_dataset_size[\"train\"],\n",
    "}\n",
    "TEST={\"model\":model_conv,\n",
    "       \"criterion\":criterion,\n",
    "       \"optimizer\":optimizer_conv,\n",
    "       \"scheduler\":exp_lr_scheduler,\n",
    "        \"learning_dataloader\":learning_dataloader[\"test\"],\n",
    "        \"learning_dataset_size\":learning_dataset_size[\"test\"],\n",
    "}\n",
    "\n",
    "train_test_model(TRAIN,TEST,epoch,Path)\n",
    "\n",
    "TXT.print_log(\"---------------------------\")\n",
    "TXT.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATAFRAME.to_csv('dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
